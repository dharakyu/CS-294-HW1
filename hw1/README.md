# Build a QA Skill for a Wikidata Domain with Genie

In this homework, you will learn to use Genie and build a question-answering skill for a Wikidata domain of your choice. 
**Please sign up for the domain [here](https://docs.google.com/spreadsheets/d/1iibWKklrBbH6JD7vJfaMHyMbsipxHrFw6OlCoORdkhI/edit#gid=0?usp=sharing)**. 
Each domain can have a maximum of 2 students. 
This is a group homework, students signed up for the same domain should work together and only submit one zip file at the end.

**Please start early and budget your time accordingly!** 
This homework requires generating a large dataset and training a large neural network. 
Simply running every command once may take more than 10 hours.

## Collecting Questions
You have chosen a domain you like, you must have a lot of interesting questions for this domain yourself! 
Please write down 20 questions (per student) you would ask for this domain. Put the in the corresponding tab
of this shared [spreadsheet](https://docs.google.com/spreadsheets/d/1PtCa3jnGEeUE-pnN2rK51T9VtQDMEyKWlvwCkyQrqqA/edit?usp=sharing).

Unfortunately, with high probability, not all questions can be answered by the information 
available in Wikidata. Check the list of properties for your domain in [domains.md](./domains.md). 
(**Do not look at it before finishing the 20 questions.**)
Mark the questions that cannot be answered, and comment on why it can not be answered by Wikidata.

Now you have an idea of what kind of information Wikidata has, come up with another 20 questions (per student) 
that can be answered by Wikidata. Put them in the spreadsheet as well, with a blank row to separate with 
previous questions. These questions will serve as the evaluation set for your QA skill.

## Setup

First, you need to install Genie toolkit and its dependencies. We prepared a script 
[`install-cloud.sh`](https://github.com/stanford-oval/cs294-fall2020/blob/master/hw1/install-cloud.sh)
to install everything
in a linux cloud instance. For local testing, we recommend to install them manually. 

Genie toolkit requires`nodejs` (>=10.0) and `yarn` as a package manager. 
See [nodejs](https://nodejs.org/en/download/) and [yarn](https://classic.yarnpkg.com/en/docs/install/) for installation details. 
Genie toolkit also needs [gettext](https://www.gnu.org/software/gettext/). 
For Mac users, you can install it by `brew install gettext`. 
You can check your installation by running `node --version`, `yarn --version` and `gettext --version`.

In addition, you will need [thingpedia-cli](https://github.com/stanford-oval/thingpedia-cli),
which provides an easy way to download data from and upload data to Thingpedia. 
Run the following command to install it: 
```bash
yarn global add thingpedia-cli
```

After installation, you should get a command called `thingpedia`.
If encounter `command not found`, make sure the Yarn global bin directory
(usually `~/.yarn/bin`) is in your PATH. You can find the path with the command
`yarn global bin`.

```bash
export PATH=~/.yarn/bin:$PATH
```

Now, you can install Genie toolkit with the following command: 
```bash
git clone https://github.com/stanford-oval/genie-toolkit
cd genie-toolkit
git checkout wip/wikidata-single-turn
yarn
```

## Testing Genie with Your Domain
The experiment uses the starter code in `genie-toolkit`. Get into the directory:
```bash
cd starter/wikidata
```

Following the instructions in [`genie-toolkit/starter/wikidata/README.md`](https://github.com/stanford-oval/genie-toolkit/blob/wip/wikidata-single-turn/starter/wikidata/README.md)
to test the quality of training data generated by Genie for your domain (skip installation instructions, as you should have done that at this point). 

Run the data synthesis with a small target pruning size (the default size will take a long time): 
```bash
make experiment=$(exp) target_pruning_size=10 datadir
```
Check the auto-generated manifest `$(exp)/manifest.tt` and synthesized sentences in `datadir/train.tsv`,
and see if the sentences make sense. 

Based on your observation, update the annotations in the `manifest.tt` by adding manual annotation override
to `MANUAL_PROPERTY_CANONICAL_OVERRIDE` in `tool/autoqa/wikidata/manual-annotations.js`.
to improve the quality of synthesized sentences.
Documentation on how the annotation works can be found in [Almond Wiki](https://wiki.almond.stanford.edu/genie/annotations).

Rerun the data synthesis by running the following command:
```bash
make clean
make experiment=$(exp) target_pruning_size=10 --mode manual datadir
``` 

Iterate your manual annotation override until you are satisfied with the synthesized sentences. 


## Running on Google Cloud Platform
This homework requires access to significant compute resources, 
including GPUs to train a neural network. 
To simplify that, all students will receive a Google Cloud Platform coupon. 
You should have received an email with instructions to redeem your coupon and apply it to your personal GCP account.

You will be responsible for creating and managing (starting, stopping) the VM instances used by this homework. 
**You will be billed while the instances are running** (and you will be responsible for charges beyond the coupon) 
so make sure you turn off any VM instance you are not using.

It is recommended to create a CPU+GPU instance, using an NVidia V100 GPU (~$2/hour)

We recommend using the Oregon (us-west-1) region, 
as it slightly cheaper and includes a larger set of available machine types, 
but any North American region should work.
See detailed instructions and a tutorial for GCP [here](./google-cloud.md).


#### Data synthesis
Install Genie toolkit and its dependencies by running `./install-cloud.sh`. 
Copy over your local changes in `tool/autoqa/wikidata/manual-annotations.js`. 
Then get into `starter/wikidata` directory, and run a full data synthesis:
```bash
make experiment=$(exp) datadir
```

#### Training
Train a model using the following 
```bash
make experiment=$(exp) train
``` 

#### Evaluation
Evaluate the trained model on synthesized dev set:
```bash
make experiment=$(exp) evaluate
```
As the dev set has the same distribution as the training set. You should see a high accuracy. 

## Annotating Your 20 Questions
Now you have a trained model, you can use it to annotate the 20 questions you came up with. 
Copy over the answerable questions from the spreadsheet and put them under `$(exp)/eval/input.txt`, 
with one question each line, and in the first line put "utterance".
 

run the annotation script: 
```bash
make experiment=$(exp) eval_set=eval annotate
```

This command will start command line interface to annotate your questions with ThingTalk.
You can find a guide to ThingTalk in [Almond Wiki](https://wiki.almond.stanford.edu/thingtalk/guide).
 
For each question, the top candidates predicted by the trained model will be provided. 
You can type in the number of the candidate to choose the correct one. 
If none of the candidate is correct, but some candidate is close to what we want, you can 
type in `e ${number}`, this will allow you to modify based on the existing prediction. 
If the model failed to predict anything useful, you will need to type in the correct ThingTalk program
from scratch. 
If you find a question cannot be represented in ThingTalk, type in `d ${comment}` (comment is optional)
to drop the sentence. 


Evaluate your model on the annotated questions:
```bash
make experiment=$(exp) eval_set=eval evaluate
```


## Submission
Your submission should include a complete dialogue agent for a new domain. 
Package the whole starter code and all generated files (datasets, trained models, etc.) 
into a zip file or tarball, then upload it to [Stanford Box](https://stanford.account.box.com/login). 

Each group should submit a text file on Canvas, and include the following information:
- the domain of the group
- SUID of each group member
- a link to the spreadsheet with the first 20 questions (make sure the link is set to be viewable for anyone in Stanford)
- link to the uploaded file on Stanford box (make sure you choose "People in your company" when creating the shared link)
- a code snippet containing the modified `MANUAL_PROPERTY_CANONICAL_OVERRIDE` in `manual-annotations.js`.
